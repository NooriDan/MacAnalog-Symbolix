{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install swig\n",
    "! pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.pprint_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQPN Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim, replay_buffer_size=10000, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, gamma=0.99, lr=0.001):\n",
    "        self.model = DQN(input_dim, output_dim)\n",
    "        self.target_model = DQN(input_dim, output_dim)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Synchronize weights initially\n",
    "        self.target_model.eval()  # Target model in evaluation mode\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Action space\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action using an epsilon-greedy strategy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.output_dim - 1)  # Explore\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state)\n",
    "            return torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in the replay buffer.\"\"\"\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences and train the model.\"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return  # Not enough experiences to train\n",
    "        \n",
    "        # Sample a batch\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        # print(states)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Compute Q-values\n",
    "        q_values = self.model(states).gather(1, actions)  # Q(s, a)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = self.target_model(next_states).max(dim=1, keepdim=True)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target network with the weights of the current model.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon for exploration-exploitation trade-off.\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Example Usage\n",
    "input_dim   = env.observation_space.shape[0]\n",
    "output_dim  = env.action_space.n\n",
    "agent = DQNAgent(input_dim, output_dim)\n",
    "\n",
    "def train_agent(agent, env, episodes, batch_size=64, update_target_interval=10):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]  # Reset the environment at the start of each episode\n",
    "        print(\"initial state: \", state)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Select and perform an action\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            # Combine terminated and truncated to determine if the episode is done\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store the experience in the replay buffer\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "            # Train the agent\n",
    "            agent.train(batch_size)\n",
    "            \n",
    "            # Transition to the next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Decay exploration rate\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        # Update target network periodically\n",
    "        if episode % update_target_interval == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        # Logging\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_reward:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "# Train the agent\n",
    "train_agent(agent, env, episodes=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, env, episodes=10):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()  # Reset the environment at the start of each episode\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Select the best action (no exploration, only exploitation)\n",
    "            action = agent.select_action(state)  \n",
    "            \n",
    "            # Take the action and observe the next state and reward\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            # Combine terminated and truncated to determine if the episode is done\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Accumulate reward for this episode\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "        # Logging the result of the test episode\n",
    "        print(f\"Test Episode {episode + 1}/{episodes}, Total Reward: {episode_reward:.2f}\")\n",
    "    \n",
    "    print(\"Testing completed!\")\n",
    "\n",
    "# Test the trained agent\n",
    "test_agent(agent, env, episodes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "trial: 1 moves = 132\n",
      "-------------------\n",
      "===================\n",
      "trial: 2 moves = 283\n",
      "-------------------\n",
      "===================\n",
      "trial: 3 moves = 395\n",
      "-------------------\n",
      "===================\n",
      "trial: 4 moves = 495\n",
      "-------------------\n",
      "===================\n",
      "trial: 5 moves = 587\n",
      "-------------------\n",
      "===================\n",
      "trial: 6 moves = 688\n",
      "-------------------\n",
      "===================\n",
      "trial: 7 moves = 770\n",
      "-------------------\n",
      "===================\n",
      "trial: 8 moves = 846\n",
      "-------------------\n",
      "===================\n",
      "trial: 9 moves = 918\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "# Initialise the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "count = 0\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "for i in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "    # print(\"action :\", action)\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        count += 1\n",
    "        observation, info = env.reset()\n",
    "        print(\"===================\")\n",
    "        print(f\"trial: {count} moves = {i}\")\n",
    "        print(\"-------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(env.action_space).n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.5, 2.5, 10.0, 10.0, 6.2831854820251465, 10.0, 1.0, 1.0],\n",
       " [-2.5, -2.5, -10.0, -10.0, -6.2831854820251465, -10.0, -0.0, -0.0])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "env.observation_space.high.tolist(), env.observation_space.low.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(2, 10)\n",
    "output = model(torch.tensor([1.0,2.0]))\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "target = torch.tensor([0.5] * 10)  # Example target tensor\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "print(model.parameters)\n",
    "\n",
    "\n",
    "# Check gradients\n",
    "for param in model.parameters():\n",
    "    print(param.grad)  # Gradients of the loss w.r.t. each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple linear model\n",
    "model = nn.Linear(2, 1)  # 2 inputs, 1 output\n",
    "input = torch.tensor([1.0, 2.0])\n",
    "target = torch.tensor([3.0])  # Scalar target\n",
    "\n",
    "# Forward pass\n",
    "output = model(input)\n",
    "\n",
    "# Compute loss\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients\n",
    "for param in model.parameters():\n",
    "    print(param.grad)  # Gradients of the loss w.r.t. each parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example data\n",
    "x_train = torch.rand((100, 2))  # 100 samples, 2 features each\n",
    "y_train = torch.rand((100, 1))  # 100 labels\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define a simple model\n",
    "model = nn.Linear(2, 1)  # Input size = 2, output size = 1\n",
    "model = DQN(2, 1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000  # Number of epochs\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0  # To track loss across batches\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
